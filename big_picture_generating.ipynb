{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import qutip as qt\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers.legacy import Nadam\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.constraints import MaxNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"TensorFlow **IS** using the GPU\")\n",
    "else:\n",
    "    print(\"TensorFlow **IS NOT** using the GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "traj_length = 10 # how many \"snapshots\" in time we take\n",
    "nof_samples_distr = 10  # how many points to sample from distribution\n",
    "checking_step = 1\n",
    "beginning_m = 5\n",
    "\n",
    "epochz = 10000\n",
    "patienc = 500\n",
    "batchsize = 512\n",
    "d = 4  # beginning dim\n",
    "\n",
    "samples = 20000\n",
    "half_sam = samples // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):  # MY CUSTOM LOSS FUNCTION - same as in Talitha's approach\n",
    "    input_shape = tf.shape(y_pred)\n",
    "\n",
    "    trace = tf.reduce_sum(tf.square(y_pred), axis=-1)  # trace shape [batchsize, 1]\n",
    "\n",
    "    trace = tf.tile(tf.reshape(trace, [input_shape[0], 1, 1]), multiples=[1, d, d])\n",
    "    matrix_form = tf.reshape(y_pred, (input_shape[0], 2, d, d))  # turn vectors into matrices\n",
    "    matrix_com = tf.complex(matrix_form[:, 0, :, :], matrix_form[:, 1, :, :])  # connect matrices into complex matrices\n",
    "\n",
    "    transpose_matrix = tf.transpose(matrix_com, perm=[0, 2, 1], conjugate=True)  # complex conjugate of matrices\n",
    "    result = tf.keras.backend.batch_dot(transpose_matrix, matrix_com)  # M.dag() * M\n",
    "    final_stuff = tf.divide(result, tf.cast(trace, tf.complex64))  # previous / trace - normalisation\n",
    "\n",
    "    finalfinal_stuff = tf.concat([tf.reshape(tf.math.real(final_stuff), (input_shape[0], -1)),\n",
    "                                  tf.reshape(tf.math.imag(final_stuff), (input_shape[0], -1))], axis=-1)  # turning\n",
    "    # it back into the vector\n",
    "\n",
    "    return tf.math.reduce_mean(tf.square(finalfinal_stuff - y_true), axis=-1)  # MSE calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_net(num_points_px, num_points_time):  # creating and compiling the network\n",
    "    net = tf.keras.models.Sequential()\n",
    "    net.add(Dense(512, input_shape=(num_points_px * num_points_time,), activation='relu', kernel_constraint=MaxNorm(3)))\n",
    "    net.add(Dropout(0.2))\n",
    "    \n",
    "    net.add(Dense(256, activation='relu', kernel_constraint=MaxNorm(3)))\n",
    "    net.add(Dropout(0.2))\n",
    "    \n",
    "    net.add(Dense(512, activation='relu', kernel_constraint=MaxNorm(3)))\n",
    "    \n",
    "    net.add(Dense(2 * d ** 2, activation='tanh'))\n",
    "\n",
    "    # Using the Adam optimizer with Nesterov momentum and the learning rate schedule\n",
    "    optimizer = Nadam(learning_rate=0.001)\n",
    "\n",
    "    net.compile(loss=custom_loss, optimizer=optimizer)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def give_back_matrix(vectr):\n",
    "    global d\n",
    "\n",
    "    # Reshape the vector into a 2D array with real and imaginary parts\n",
    "    vec = vectr.reshape(2, d**2)\n",
    "\n",
    "    # Combine the real and imaginary parts to create a complex matrix\n",
    "    matrix = vec[0, :] + 1j * vec[1, :]\n",
    "\n",
    "    # Reshape the matrix to have dimensions (d, d)\n",
    "    matrix = matrix.reshape(d, d)\n",
    "\n",
    "    # Create a Qobj using the reshaped matrix\n",
    "    return qt.Qobj(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def my_fidelity(vec1, vec2):\n",
    "    # Convert input vectors to Qobj matrices\n",
    "    vec1 = give_back_matrix(vec1)\n",
    "    vec2 = give_back_matrix(vec2)\n",
    "\n",
    "    # Check if vec1 is Hermitian\n",
    "    if vec1.isherm:\n",
    "        # Normalize vec2\n",
    "        vec2_normalized = (vec2.dag() * vec2) / (vec2.dag() * vec2).tr()\n",
    "\n",
    "        # Calculate and return the fidelity between vec1 and the normalized vec2\n",
    "        return qt.fidelity(vec1, vec2_normalized)\n",
    "    else:\n",
    "        raise ValueError('X is not Hermitian!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sample_dataset(dataset, num_points_px, num_points_time):\n",
    "    time_indices = np.linspace(0, dataset.shape[0] - 1, num_points_time, dtype=int)\n",
    "    px_indices = np.linspace(0, dataset.shape[1] - 1, num_points_px, dtype=int)\n",
    "\n",
    "    sampled_dataset_2d = dataset[np.ix_(time_indices, px_indices)]\n",
    "\n",
    "    sampled_dataset = sampled_dataset_2d.flatten()\n",
    "\n",
    "    return sampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_points_samples(num_samples):\n",
    "    samplez = []\n",
    "\n",
    "    # Iterate over the number of samples\n",
    "    for i in range(num_samples):\n",
    "        # Load trajectory data from the file\n",
    "        trajectory = np.load(f\"new_data/drawn_points/one_sample{i}.npy\")\n",
    "\n",
    "        # Convert the list of lists to a 2D NumPy array\n",
    "        data_array = np.array(trajectory)\n",
    "\n",
    "        # Flatten the trajectory and append it to the samples list\n",
    "        samplez.append(data_array)\n",
    "\n",
    "    return samplez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_states1 = np.load('new_data/states.npy')\n",
    "y_states = y_states1[:half_sam, :]\n",
    "y_states_valid = y_states1[half_sam:, :]\n",
    "\n",
    "y_bins1 = create_points_samples(samples)\n",
    "\n",
    "y_bins = y_bins1[:half_sam]\n",
    "y_bins_valid = y_bins1[half_sam:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_bins = np.float32(y_bins)\n",
    "y_bins_valid = np.float32(y_bins_valid)\n",
    "\n",
    "y_states = np.float32(y_states)\n",
    "y_states_valid = np.float32(y_states_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "in_fidelities = []\n",
    "for num_points_px in np.arange(beginning_m, nof_samples_distr+checking_step, checking_step, dtype=int):\n",
    "    in_fidelities_1 = []\n",
    "    for num_points_time in np.arange(beginning_m, traj_length+checking_step, checking_step, dtype=int):\n",
    "        print(f\"DOING {num_points_px} POINTS IN P(x), {num_points_time} POINTS IN TIME ... \")\n",
    "        beginning = time.time()\n",
    "\n",
    "        y_bins_final = [sample_dataset(x, num_points_px, num_points_time) for x in y_bins]\n",
    "        y_bins_valid_final = [sample_dataset(x, num_points_px, num_points_time) for x in y_bins_valid]\n",
    "\n",
    "        y_bins_final = np.array(y_bins_final)\n",
    "        y_bins_valid_final = np.array(y_bins_valid_final)\n",
    "\n",
    "        model = init_net(num_points_px, num_points_time)  # creating the network\n",
    "\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patienc, verbose=1, mode='min')\n",
    "\n",
    "        callbackz = [early_stop, tf.keras.callbacks.ModelCheckpoint(filepath=f'Models/best_model_draw.h5', monitor='val_loss',\n",
    "                                                save_best_only=True, mode='min')] \n",
    "        \n",
    "        # early stopping and saving the best model, to use in validation set\n",
    "\n",
    "        history = model.fit(y_bins_final, y_states, batch_size=batchsize, epochs=epochz,\n",
    "                            validation_data=(y_bins_valid_final, y_states_valid),\n",
    "                            callbacks=callbackz)  # training the network\n",
    "\n",
    "        # get the number of epochs at which training stopped\n",
    "        num_epochs = early_stop.stopped_epoch + 1\n",
    "        \n",
    "        model1 = tf.keras.models.load_model(f'Models/best_model_draw.h5',\n",
    "                                    custom_objects={'custom_loss': custom_loss})  # loading the best model for the\n",
    "        # validation set\n",
    "\n",
    "        avg_infidelities = []\n",
    "        validation_predict = model1.predict(y_bins_valid_final)  # use the best model on valid_data\n",
    "        fidelities = [my_fidelity(y_states_valid[i, :], validation_predict[i, :]) for i in range(half_sam)]\n",
    "        print(f\"For {num_points_px} and {num_points_time} score: {1 - np.average(fidelities)}\")  # average INfidelity in validation set\n",
    "        in_fidelities_1.append(1 - np.average(fidelities))\n",
    "\n",
    "        print(f\"--- {time.time() - beginning} seconds ---\")  # how much time did it take\n",
    "\n",
    "        # create a directory for saved data if it doesn't exist\n",
    "        if not os.path.exists('training_data/run_1'):\n",
    "            os.makedirs('training_data/run_1')\n",
    "\n",
    "        # save the data to a file in the training_data directory\n",
    "        with open(f'training_data/run_1/data_{num_points_px}_px_{num_points_time}_t.txt', 'w') as f:\n",
    "            f.write(f\"Num_epochs: {num_epochs}\\n\")\n",
    "            f.write(f\"Time of training: {time.time() - beginning} s \\n\")\n",
    "            f.write(f\"Avg infidelity: {1 - np.average(fidelities)} \\n\")\n",
    "            f.write(f\"Median infidelity: {1 - np.median(fidelities)} \\n\")\n",
    "            f.write(f\"Std dev. infidelity: {np.std(fidelities)} \\n\")\n",
    "            f.write(f\"Min. infidelity: {1 - np.max(fidelities)} \\n\")\n",
    "            f.write(f\"Max. infidelity: {1 - np.min(fidelities)} \\n\")\n",
    "\n",
    "\n",
    "        # Clear the Tensorflow session and release the memory\n",
    "        K.clear_session()\n",
    "    \n",
    "    in_fidelities.append(in_fidelities_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(in_fidelities)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Convert in_fidelities to a NumPy array\n",
    "in_fidelities_array = np.array(in_fidelities)\n",
    "\n",
    "# Create the x and y tick labels\n",
    "x_tick_labels = np.arange(beginning_m, nof_samples_distr+checking_step, checking_step, dtype=int)\n",
    "y_tick_labels = np.arange(beginning_m, traj_length+checking_step, checking_step, dtype=int)\n",
    "\n",
    "# Create the 2D plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(in_fidelities_array, origin='lower', cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Infidelity')\n",
    "\n",
    "# make colors logarithmic\n",
    "\n",
    "# Set the x and y ticks and labels\n",
    "plt.xticks(np.arange(len(x_tick_labels)), x_tick_labels)\n",
    "plt.yticks(np.arange(len(y_tick_labels)), y_tick_labels)\n",
    "plt.xlabel('Number of points in P(x)')\n",
    "plt.ylabel('Number of points in time')\n",
    "plt.savefig(\"AAA_punkty_1-10.png\")\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# x_ax = np.arange(0, len(history.history['loss']), 50)\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(x_ax, history.history['val_loss'][::50])  # plot both losses during training\n",
    "# plt.title('Model loss')\n",
    "# plt.ylabel('loss functions')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.yscale('log')\n",
    "# plt.legend(['training set', 'validation set'], loc='upper left')\n",
    "# plt.savefig('new_loss_functions_hyperparams.png', bbox_inches='tight', dpi=300)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('licencjat')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a09f34fd78ee26f2d768faaac4cab2723b396d6d6944281ea7013083524ca8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}